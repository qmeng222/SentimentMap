{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd06d078",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Reddit Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564bcfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.error import APIConnectionError, APIError, RateLimitError\n",
    "from typing import List, Dict, Generator, Optional\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import praw\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d6de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "load_dotenv(\".env\")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "    client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "    user_agent=f\"script:test:0.0.1 (by u/yourusername)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf295098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "def num_tokens_from_messages(messages, model):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657b89c",
   "metadata": {},
   "source": [
    "## Getting Reddit Comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "672da08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_COLUMNS = [\"subreddit\", \"submission_id\", \"score\", \"comment_body\"]\n",
    "# filename, subreddits = \"cities.csv\", [\n",
    "#     \"NYC\",\n",
    "#     \"Seattle\",\n",
    "#     \"LosAngeles\",\n",
    "#     \"Chicago\",\n",
    "#     \"Austin\",\n",
    "#     \"Portland\",\n",
    "#     \"SanFrancisco\",\n",
    "#     \"Boston\",\n",
    "#     \"Houston\",\n",
    "#     \"Atlanta\",\n",
    "#     \"Philadelphia\",\n",
    "#     \"Denver\",\n",
    "#     \"SeattleWa\",\n",
    "#     \"Dallas\",\n",
    "#     \"WashingtonDC\",\n",
    "#     \"SanDiego\",\n",
    "#     \"Pittsburgh\",\n",
    "#     \"Phoenix\",\n",
    "#     \"Minneapolis\",\n",
    "#     \"Orlando\",\n",
    "#     \"Nashville\",\n",
    "#     \"StLouis\",\n",
    "#     \"SaltLakeCity\",\n",
    "#     \"Columbus\",\n",
    "#     \"Raleigh\",\n",
    "# ]\n",
    "\n",
    "# OTHER POTENTIAL SUBREDDITS TO TRY:\n",
    "# filename, subreddits = \"iphone_v_android.csv\", [\"iphone\", \"Android\"]\n",
    "# filename, subreddits = \"startrek_v_starwars.csv\", [\"startrek\", \"StarWars\"]\n",
    "filename, subreddits = \"epl_top_8.csv\", [\"reddevils\", \"LiverpoolFC\", \"chelseafc\", \"Gunners\", \"coys\", \"MCFC\", \"Everton\", \"NUFC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f8bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for fetching comments from submissions\n",
    "def comment_generator(submission) -> Generator:\n",
    "    # Do not bother expanding MoreComments (follow-links)\n",
    "    for comment in submission.comments.list():\n",
    "        if hasattr(comment, \"body\") and comment.body != \"[deleted]\" and comment.body != \"[removed]\":\n",
    "            yield (comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "576e4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_comments(\n",
    "    filename: str,\n",
    "    target_comments_per_subreddit: int,\n",
    "    max_comments_per_submission: int,\n",
    "    max_comment_length: int,\n",
    "    reddit: praw.Reddit,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect comments from the top submissions in each subreddit.\n",
    "\n",
    "    Cache results at cache_filename.\n",
    "\n",
    "    Return a dataframe with columns: subreddit, submission_id, score, comment_body\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename, index_col=\"id\")\n",
    "        assert df.columns.tolist() == DF_COLUMNS\n",
    "    except FileNotFoundError:\n",
    "        df = pd.DataFrame(columns=DF_COLUMNS)\n",
    "\n",
    "    # dict like {comment_id -> {column -> value}}\n",
    "    records = df.to_dict(orient=\"index\")\n",
    "\n",
    "    for subreddit_index, subreddit_name in enumerate(subreddits):\n",
    "        print(f\"Processing Subreddit: {subreddit_name}\")\n",
    "\n",
    "        processed_comments_for_subreddit = len(df[df[\"subreddit\"] == subreddit_name])\n",
    "\n",
    "        if processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
    "            print(f\"Enough comments fetched for {subreddit_name}, continuing to next subreddit.\")\n",
    "            continue\n",
    "\n",
    "        # `top`` is a generator, grab submissions until we break (within this loop).\n",
    "        for submission in reddit.subreddit(subreddit_name).top(time_filter=\"month\"):\n",
    "            if processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
    "                break\n",
    "\n",
    "            # The number of comments that we already have for this subreddit\n",
    "            processed_comments_for_submission = len(df[df[\"submission_id\"] == submission.id])\n",
    "\n",
    "            for comment in comment_generator(submission):\n",
    "                if processed_comments_for_submission >= max_comments_per_submission or processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
    "                    break\n",
    "\n",
    "                if comment.id in records:\n",
    "                    print(f\"Skipping comment {subreddit_name}-{submission.id}-{comment.id} because we already have it\")\n",
    "                    continue\n",
    "\n",
    "                body = comment.body[:max_comment_length].strip()\n",
    "                records[comment.id] = {\"subreddit\": subreddit_name, \"submission_id\": submission.id, \"comment_body\": body}\n",
    "\n",
    "                processed_comments_for_subreddit += 1\n",
    "                processed_comments_for_submission += 1\n",
    "\n",
    "            # Once per post write to disk.\n",
    "            print(f\"CSV rewritten with {len(records)} rows.\\n\")\n",
    "            df = pd.DataFrame.from_dict(records, orient=\"index\", columns=DF_COLUMNS)\n",
    "            df.to_csv(filename, index_label=\"id\")\n",
    "\n",
    "    print(\"Completed.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e3819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b96dc77",
   "metadata": {},
   "source": [
    "## Run Everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0c8cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subreddit: reddevils\n",
      "CSV rewritten with 10 rows.\n",
      "\n",
      "CSV rewritten with 20 rows.\n",
      "\n",
      "CSV rewritten with 30 rows.\n",
      "\n",
      "CSV rewritten with 40 rows.\n",
      "\n",
      "CSV rewritten with 50 rows.\n",
      "\n",
      "Processing Subreddit: LiverpoolFC\n",
      "CSV rewritten with 60 rows.\n",
      "\n",
      "CSV rewritten with 70 rows.\n",
      "\n",
      "CSV rewritten with 80 rows.\n",
      "\n",
      "CSV rewritten with 90 rows.\n",
      "\n",
      "CSV rewritten with 100 rows.\n",
      "\n",
      "Processing Subreddit: chelseafc\n",
      "CSV rewritten with 110 rows.\n",
      "\n",
      "CSV rewritten with 120 rows.\n",
      "\n",
      "CSV rewritten with 130 rows.\n",
      "\n",
      "CSV rewritten with 140 rows.\n",
      "\n",
      "CSV rewritten with 150 rows.\n",
      "\n",
      "Processing Subreddit: Gunners\n",
      "CSV rewritten with 160 rows.\n",
      "\n",
      "CSV rewritten with 170 rows.\n",
      "\n",
      "CSV rewritten with 180 rows.\n",
      "\n",
      "CSV rewritten with 190 rows.\n",
      "\n",
      "CSV rewritten with 200 rows.\n",
      "\n",
      "Processing Subreddit: coys\n",
      "CSV rewritten with 210 rows.\n",
      "\n",
      "CSV rewritten with 220 rows.\n",
      "\n",
      "CSV rewritten with 230 rows.\n",
      "\n",
      "CSV rewritten with 240 rows.\n",
      "\n",
      "CSV rewritten with 250 rows.\n",
      "\n",
      "Processing Subreddit: MCFC\n",
      "CSV rewritten with 260 rows.\n",
      "\n",
      "CSV rewritten with 270 rows.\n",
      "\n",
      "CSV rewritten with 280 rows.\n",
      "\n",
      "CSV rewritten with 290 rows.\n",
      "\n",
      "CSV rewritten with 300 rows.\n",
      "\n",
      "Processing Subreddit: Everton\n",
      "CSV rewritten with 310 rows.\n",
      "\n",
      "CSV rewritten with 320 rows.\n",
      "\n",
      "CSV rewritten with 330 rows.\n",
      "\n",
      "CSV rewritten with 340 rows.\n",
      "\n",
      "CSV rewritten with 350 rows.\n",
      "\n",
      "Processing Subreddit: NUFC\n",
      "CSV rewritten with 360 rows.\n",
      "\n",
      "CSV rewritten with 370 rows.\n",
      "\n",
      "CSV rewritten with 380 rows.\n",
      "\n",
      "CSV rewritten with 390 rows.\n",
      "\n",
      "CSV rewritten with 400 rows.\n",
      "\n",
      "Completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>score</th>\n",
       "      <th>comment_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jkr0z7j</th>\n",
       "      <td>reddevils</td>\n",
       "      <td>13lqbis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Source: https://www.instagram.com/p/Csa1f8NM7-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jkr1710</th>\n",
       "      <td>reddevils</td>\n",
       "      <td>13lqbis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I always remember when he said no one would bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jkr0xqv</th>\n",
       "      <td>reddevils</td>\n",
       "      <td>13lqbis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poor guy with all the injuries - wishing him a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jkr1qh5</th>\n",
       "      <td>reddevils</td>\n",
       "      <td>13lqbis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"I've said before that I found it hard to even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jkr1v2g</th>\n",
       "      <td>reddevils</td>\n",
       "      <td>13lqbis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Even if injured, name him on the bench for fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jl7ubhu</th>\n",
       "      <td>NUFC</td>\n",
       "      <td>13p50iu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Someone make this our subreddit banner please\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jl7qp4f</th>\n",
       "      <td>NUFC</td>\n",
       "      <td>13p50iu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nick Pope absolutely makes this photo epic! LOL!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jl7q02h</th>\n",
       "      <td>NUFC</td>\n",
       "      <td>13p50iu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hands down best pic yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jl7puyp</th>\n",
       "      <td>NUFC</td>\n",
       "      <td>13p50iu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Murphy is saying it all for me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jl7q67n</th>\n",
       "      <td>NUFC</td>\n",
       "      <td>13p50iu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Murphy just has it. What a lad.. On another no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subreddit submission_id score  \\\n",
       "jkr0z7j  reddevils       13lqbis   NaN   \n",
       "jkr1710  reddevils       13lqbis   NaN   \n",
       "jkr0xqv  reddevils       13lqbis   NaN   \n",
       "jkr1qh5  reddevils       13lqbis   NaN   \n",
       "jkr1v2g  reddevils       13lqbis   NaN   \n",
       "...            ...           ...   ...   \n",
       "jl7ubhu       NUFC       13p50iu   NaN   \n",
       "jl7qp4f       NUFC       13p50iu   NaN   \n",
       "jl7q02h       NUFC       13p50iu   NaN   \n",
       "jl7puyp       NUFC       13p50iu   NaN   \n",
       "jl7q67n       NUFC       13p50iu   NaN   \n",
       "\n",
       "                                              comment_body  \n",
       "jkr0z7j  Source: https://www.instagram.com/p/Csa1f8NM7-...  \n",
       "jkr1710  I always remember when he said no one would bo...  \n",
       "jkr0xqv  Poor guy with all the injuries - wishing him a...  \n",
       "jkr1qh5  \"I've said before that I found it hard to even...  \n",
       "jkr1v2g  Even if injured, name him on the bench for fin...  \n",
       "...                                                    ...  \n",
       "jl7ubhu  Someone make this our subreddit banner please\\...  \n",
       "jl7qp4f   Nick Pope absolutely makes this photo epic! LOL!  \n",
       "jl7q02h                            Hands down best pic yet  \n",
       "jl7puyp                     Murphy is saying it all for me  \n",
       "jl7q67n  Murphy just has it. What a lad.. On another no...  \n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_COMMENTS_PER_SUBREDDIT = 50\n",
    "MAX_COMMENTS_PER_SUBMISSION = 10\n",
    "MAX_COMMENT_LENGTH = 2000\n",
    "\n",
    "collect_comments(\n",
    "    filename=filename,\n",
    "    target_comments_per_subreddit=TARGET_COMMENTS_PER_SUBREDDIT,\n",
    "    max_comments_per_submission=MAX_COMMENTS_PER_SUBMISSION,\n",
    "    max_comment_length=MAX_COMMENT_LENGTH,\n",
    "    reddit=reddit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b296e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
